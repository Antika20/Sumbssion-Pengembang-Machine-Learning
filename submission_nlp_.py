# -*- coding: utf-8 -*-
"""Submission NLP .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o5B8fm79W0cDS2bTcJf8fhNef5Oe3JVH

# Nama : Antika Orinda
# XP : 10.179
#BERGABUNG : SEJAK 29 SEPTEMBER 2020
#LOKASI : KOTA BEKASI,JAWA BARAT

# Instal Kaggle
"""

# install kaggle package
!pip install -q kaggle

"""# Upload Kaggle Json"""

# upload kaggle.json
from google.colab import files
files.upload()

"""# Make directory & Change Permission Kaggle"""

# make directory and change permission
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""# Download Dataset from Kaggle """

# download dataset, choose 'copy api command' from kaggle dataset
!kaggle datasets download -d hgultekin/bbcnewsarchive

"""# Ekstrak ZIP """

from zipfile import ZipFile
file_name = "/content/bbcnewsarchive.zip"

with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print("done")

"""# Prepare & Load Dataset"""

# load dataset
import pandas as pd
df = pd.read_csv('/content/bbc-news-data.csv', sep='\t')
df.head(10)

"""# View Columns"""

# data columns
df.columns

"""# View total data -> Jumlah isi data serta category data tersebut"""

# total data
df.shape

"""# View Category & Jumlah Data """

# categories
df.category.value_counts()

"""# Find Missing Value """

df.isnull()

"""# Find Missing Values in list Dataset"""

df.isnull().sum()

"""# View Total Number of Missing Value """

#Total Number of Missing NA
df.isnull().sum().sum()

"""# Delete Column yang tidak terpakai """

# delete columns (unused column)
df_new = df.drop(columns=['filename'])
df_new

"""# One Hot Encoding Before Cleansing Data """

# One-hot encoding

categorynews = pd.get_dummies(df.category)
df_baru = pd.concat([df, categorynews], axis=1)
df_baru = df_baru.drop(columns='category')
df_baru

"""# CLEANSING DATA

# Import Module & Package you need
"""

# import and download package
import nltk, os, re, string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

nltk.download('wordnet')
nltk.download('stopwords')

"""# Lets Time the Cleansing âœŒ"""

# lower-case all characters
df_new.title = df_new.title.apply(lambda x: x.lower())
df_new.content = df_new.content.apply(lambda x: x.lower())

# removing functuation
def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    df_new.title = df_new.title.apply(lambda x: cleaner(x))
    df_new.content = df_new.content.apply(lambda x: lem(x))

## lematization
lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    df_new.title = df_new.title.apply(lambda x: lem(x))
    df_new.content = df_new.content.apply(lambda x: lem(x))

# removing number
def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    df_new['title'].apply(rem_numbers)
    df_new['content'].apply(rem_numbers)

# removing stopword
st_words = stopwords.words()
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    df_new.title = df_new.title.apply(lambda x: stopword(x))
    df_new.content = df_new.content.apply(lambda x: lem(x))

"""# View Data After Cleansing ðŸ¤"""

# view data after cleansing
df_new.head(10)

"""# View One- Hot Encoding  After Cleansing ðŸ•µ"""

# data category one-hot-encoding
category = pd.get_dummies(df_new.category)
df_News = pd.concat([df_new, category], axis=1)
df_News = df_News.drop(columns='category')
df_News.head(10)

"""# Convert Data Frame to Numpy Array with Values âš¡"""

textNews = df_News['title'].values + '' + df_News['content'].values
labelNews = df_News[['business', 'entertainment', 'politics', 'sport', 'tech']].values

"""# View Result âœ"""

textNews

labelNews

"""# Split Dataset to Train & Test ðŸ”¥"""

from sklearn.model_selection import train_test_split
textNews_latih, textNews_test, labelNews_latih, labelNews_test = train_test_split(textNews, labelNews, test_size=0.2)

"""# Lets Create Tokenizer ðŸˆ¶"""

# Tokenizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words= 5000, oov_token='x')
tokenizer.fit_on_texts(textNews_latih) 
tokenizer.fit_on_texts(textNews_test)
 
sekuens_train = tokenizer.texts_to_sequences(textNews_latih)
sekuens_test = tokenizer.texts_to_sequences(textNews_test)
 
padded_latih = pad_sequences(sekuens_train)
padded_test = pad_sequences(sekuens_test)

"""# Create Model LSTM ã€°"""

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

"""# USE Callbacks 

for To stop Epoch Training Cause the epoch haved crossed the accuracy I wanted
"""

# callback
class NewsCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.90):
      self.model.stop_training = True
      print("\nHentikan Trainingnya Sekarang , sudah melewati batas  > 90%!")
callbacks = NewsCallback()

"""# FITTING MODEL ðŸŒ‹"""

num_epochs = 30
history = model.fit(padded_latih, labelNews_latih, epochs=num_epochs, 
                    validation_data=(padded_test, labelNews_test), verbose=1,callbacks=[callbacks])

"""# VISUALISASI EPOCH ACCURACCY & LOSS"""

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model NLP ')
plt.ylabel('accuracy NLP')
plt.xlabel('epoch NLP')
plt.legend(['training', 'testing'], loc='upper left')
plt.show()

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Akurasi Model NLP ')
plt.ylabel('accuracy NLP')
plt.xlabel('epoch NLP')
plt.legend(['training', 'testing'], loc='upper left')
plt.show()

"""# Conffusion Matriks ðŸ¤Ÿ"""

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Defining list with labels
labels = ['bussiness','entertaiment','politics','sport','tech']


# Check point
# Showing labels
print(labels)

# Generating Numpy array with True classes' indexes
y_true = np.random.randint(low=0, high=5, size=100, dtype=int)


# Check point
# Shwoing array
print(y_true)

# Calculating number of samples for every class
# Iterating all classes' indexes in 'y_true' array
# Using Numpy function 'unique'
# Returning sorted unique elements and their frequencies
classesIndexes, classesFrequency = np.unique(y_true, return_counts=True)


# Printing frequency (number of samples) for every class
print('classes indexes:' , classesIndexes)
print('\n')
print('classes frequency:', classesFrequency)

# Commented out IPython magic to ensure Python compatibility.
# Magic function that renders the figure in a jupyter notebook
# instead of displaying a figure object
# %matplotlib inline


# Setting default size of the plot
plt.rcParams['figure.figsize'] = (10.0, 7.0)


# Plotting histogram of 3 classes with their number of samples
# Defining a figure object 
figure = plt.figure()

# Plotting Bar chart
plt.bar(classesIndexes, classesFrequency, align='center', alpha=0.6)


# Giving name to Y axis
plt.ylabel('Class frequency', fontsize=16)


# Giving names to every Bar along X axis
plt.xticks(classesIndexes, labels, fontsize=16)


# Giving name to the plot
plt.title('Histogram', fontsize=20)


# Saving the plot
figure.savefig('histogram.png', transparent=True, dpi=500)


# Showing the plot
plt.show()

# Making copy of array with True classes' indexes
y_predicted = np.copy(y_true)

# Choosing randomly 25% of classes to be changed
ii = np.random.randint(low=0, high=len(y_true), size=int(0.25 * len(y_true)), dtype=int)


# Check point
# Showing chosen indexes
print(ii)

# Iterating chosen indexes and replacing them with other classes' indexes
for i in ii:
    # Generating new class index
    y_predicted[i] = np.random.randint(low=0, high=3, dtype=int)
    
    
    # Check point
    # Showing difference between True classes' indexes and Predicted ones
    print('index = {0:2d}, True class => {1}, {2} <= Predicted class'.
          format(i, y_true[i], y_predicted[i]))

# Confusion Matrix is a two dimensional matrix that visualizes the performance,
# and makes it easy to see confusion between classes,
# by providing a picture of interrelation

# Each row represents a number of actual, True class
# Each column represents a number of predicted class


# Computing Confusion Matrix to evaluate accuracy of classification
c_m = confusion_matrix(y_true, y_predicted)

# Showing Confusion Matrix in form of 2D Numpy array
print(c_m)

# Commented out IPython magic to ensure Python compatibility.
# Magic function that renders the figure in a jupyter notebook
# instead of displaying a figure object
# %matplotlib inline


# Setting default size of the plot
# Setting default fontsize used in the plot
plt.rcParams['figure.figsize'] = (10.0, 9.0)
plt.rcParams['font.size'] = 20


# Implementing visualization of Confusion Matrix
display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels)


# Plotting Confusion Matrix
# Setting colour map to be used
display_c_m.plot(cmap='OrRd', xticks_rotation=25)
# Other possible options for colour map are:
# 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'


# Setting fontsize for xticks and yticks
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)


# Giving name to the plot
plt.title('Confusion Matrix', fontsize=24)


# Saving plot
plt.savefig('confusion_matrix.png', transparent=True, dpi=500)


# Showing the plot
plt.show()

# Showing the main classification metrics
print(classification_report(y_true, y_predicted))